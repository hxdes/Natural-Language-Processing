# -*- coding: utf-8 -*-
"""NLP ASSIGNMENT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gban753KaBVjc3eNWTCBFWn26HhJF-_o
"""

import pandas as pd


# Load dataset
df = pd.read_csv('/content/Reviews.csv', on_bad_lines='skip')

# View basic info
print("Shape:", df.shape)
print("\nColumn Types:\n", df.dtypes)

# Check for null values
print("\nMissing Values:\n", df.isnull().sum())

# Preview first 5 rows
df.head()

# Drop rows with missing Text or Score
df_clean = df.dropna(subset=['Text', 'Score'])

df_clean['Score'] = df_clean['Score'].astype(int)

# Create sentiment labels
def map_sentiment(score):
    if score in [1, 2]:
        return 'negative'
    elif score == 3:
        return 'neutral'
    else:
        return 'positive'

df_clean['Sentiment'] = df_clean['Score'].apply(map_sentiment)

# Final cleaned DataFrame (we'll use only Text + Sentiment for now)
df_final = df_clean[['Text', 'Sentiment']]

# Show class distribution
print(df_final['Sentiment'].value_counts())
df_final.head()

import matplotlib.pyplot as plt
import seaborn as sns

# Add columns for review length
df_final['text_length_char'] = df_final['Text'].apply(len)
df_final['text_length_words'] = df_final['Text'].apply(lambda x: len(x.split()))

# Plot histograms
plt.figure(figsize=(12, 5))

# Characters
plt.subplot(1, 2, 1)
sns.histplot(data=df_final, x='text_length_char', bins=50, kde=True)
plt.title('Review Length (Characters)')
plt.xlabel('Characters')
plt.ylabel('Frequency')

# Words
plt.subplot(1, 2, 2)
sns.histplot(data=df_final, x='text_length_words', bins=50, kde=True, color='orange')
plt.title('Review Length (Words)')
plt.xlabel('Words')
plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

plt.figure(figsize=(6, 4))
sns.countplot(data=df_final, x='Sentiment', order=['negative', 'neutral', 'positive'])
plt.title('Sentiment Class Distribution')
plt.xlabel('Sentiment')
plt.ylabel('Count')
plt.show()

import numpy as np

# Bring back Score column
df_final['Score'] = df_clean['Score']

# Group by length buckets
df_final['length_bucket'] = pd.cut(df_final['text_length_words'], bins=[0, 50, 100, 150, 200, 500, 1000, np.inf])
length_scores = df_final.groupby('length_bucket')['Score'].mean().reset_index()

# Plot
plt.figure(figsize=(10, 5))
sns.barplot(data=length_scores, x='length_bucket', y='Score')
plt.title('Average Score by Review Length')
plt.xlabel('Review Length Bucket (words)')
plt.ylabel('Average Score')
plt.xticks(rotation=45)
plt.show()

"""NLTK-Based Classical NLP Preprocessing"""

import nltk
import os

# Set a fixed path and ensure directory exists
nltk_path = "/content/nltk_data"
os.makedirs(nltk_path, exist_ok=True)
nltk.data.path.append(nltk_path)

# Re-download resources to that path
nltk.download('punkt', download_dir=nltk_path)
nltk.download('stopwords', download_dir=nltk_path)
nltk.download('averaged_perceptron_tagger', download_dir=nltk_path)

!pip install -U spacy

# Download the English model
!python -m spacy download en_core_web_sm

# Load SpaCy
import spacy
nlp = spacy.load("en_core_web_sm")

# Sample review
sample_text = df_final.iloc[0]['Text']

# Process text
doc = nlp(sample_text)

# Extract tokens (no stopwords or punct)
tokens = [token.text for token in doc if not token.is_stop and not token.is_punct]

# Lemmas
lemmas = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]

# POS tags
pos_tags = [(token.text, token.pos_) for token in doc if not token.is_punct]

# Show results
print("Original Text:\n", sample_text)
print("\nTokens:\n", tokens)
print("\nLemmas:\n", lemmas)
print("\nPOS Tags:\n", pos_tags[:15])  # limit to 15 for readability

def extract_adj_noun_phrases(text):
    doc = nlp(text)
    pairs = []
    for i in range(len(doc) - 1):
        # Look for pattern: ADJ followed by NOUN
        if doc[i].pos_ == "ADJ" and doc[i+1].pos_ == "NOUN":
            pairs.append(f"{doc[i].text} {doc[i+1].text}")
    return pairs

sample_text = df_final.iloc[0]['Text']
adj_noun_phrases = extract_adj_noun_phrases(sample_text)

print("Extracted Opinion Phrases:\n", adj_noun_phrases)

from collections import Counter

# Choose how many reviews you want to process (e.g., 1000)
N = 1000

# Initialize counters
positive_phrases = Counter()
neutral_phrases = Counter()
negative_phrases = Counter()

# Loop through first N rows
for _, row in df_final.iloc[:N].iterrows():
    phrases = extract_adj_noun_phrases(row['Text'])
    if row['Sentiment'] == 'positive':
        positive_phrases.update(phrases)
    elif row['Sentiment'] == 'neutral':
        neutral_phrases.update(phrases)
    elif row['Sentiment'] == 'negative':
        negative_phrases.update(phrases)

# Show top 10 opinion phrases in each class
print("\n Top Positive Opinion Phrases:")
print(positive_phrases.most_common(10))

print("\n Top Neutral Opinion Phrases:")
print(neutral_phrases.most_common(10))

print("\n Top Negative Opinion Phrases:")
print(negative_phrases.most_common(10))

!pip install -q transformers datasets accelerate

# Choose sample size per class based on smallest class
sample_size = df_final['Sentiment'].value_counts().min()  # You can also hardcode to 200 for faster testing

# Sample equally from each sentiment class
df_balanced = df_final.groupby('Sentiment', group_keys=False).apply(lambda x: x.sample(sample_size, random_state=42))

# Map sentiment labels to integers
label_map = {'negative': 0, 'neutral': 1, 'positive': 2}
df_balanced['label'] = df_balanced['Sentiment'].map(label_map)

# Check result
print(df_balanced['Sentiment'].value_counts())
print(df_balanced.head())

from transformers import DistilBertTokenizerFast

tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')

# Tokenize the reviews
X = list(df_balanced['Text'])
y = list(df_balanced['label'])

# Tokenize with padding & truncation
encodings = tokenizer(X, truncation=True, padding=True, max_length=256)

from transformers import AutoTokenizer, AutoModelForSequenceClassification

# Load tiny BERT model & tokenizer
tokenizer = AutoTokenizer.from_pretrained("prajjwal1/bert-tiny")
model = AutoModelForSequenceClassification.from_pretrained("prajjwal1/bert-tiny", num_labels=3)

X = list(df_balanced['Text'])
y = list(df_balanced['label'])

encodings = tokenizer(X, truncation=True, padding=True, max_length=128)

import torch
from torch.utils.data import Dataset

class ReviewDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        return {
            'input_ids': torch.tensor(self.encodings['input_ids'][idx]),
            'attention_mask': torch.tensor(self.encodings['attention_mask'][idx]),
            'labels': torch.tensor(self.labels[idx])
        }

    def __len__(self):
        return len(self.labels)

dataset = ReviewDataset(encodings, y)

# Train-test split
from sklearn.model_selection import train_test_split
train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size

train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=2,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    warmup_steps=20,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10
)

from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
)

trainer.train()

# Get raw predictions
preds_output = trainer.predict(val_dataset)

# Get predicted label IDs
preds = preds_output.predictions.argmax(axis=1)

# True labels
true = preds_output.label_ids

from sklearn.metrics import classification_report, accuracy_score

# Metrics
acc = accuracy_score(true, preds)
print(f"Accuracy: {acc:.4f}")

# Classification report
target_names = ['negative', 'neutral', 'positive']
print(classification_report(true, preds, target_names=target_names))

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

cm = confusion_matrix(true, preds)

plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt="d", xticklabels=target_names, yticklabels=target_names, cmap="Blues")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

import numpy as np
import pandas as pd
import re
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# ---------- 1) Get predictions on the validation split ----------
pred_out = trainer.predict(val_dataset)
logits = pred_out.predictions
pred_ids = logits.argmax(axis=1)
true_ids = pred_out.label_ids

# Get softmax confidences
def softmax(x, axis=1):
    x = x - x.max(axis=axis, keepdims=True)
    e = np.exp(x)
    return e / e.sum(axis=axis, keepdims=True)
probs = softmax(logits)

# Rebuild validation texts/labels from indices (Subset preserves order)
val_indices = val_dataset.indices  # indices into original dataset order
val_texts = [X[i] for i in val_indices]
val_true = [y[i] for i in val_indices]

# Sanity check ordering
assert len(val_texts) == len(true_ids) == len(pred_ids)

# ---------- 2) Build analysis DataFrame ----------
inv_label_map = {v:k for k,v in label_map.items()}

def word_len(t):
    return len(re.findall(r"\b\w+\b", t))

NEGATION_RE = re.compile(r"\b(no|not|never|nothing|nowhere|hardly|barely|scarcely|isn\'t|aren\'t|wasn\'t|weren\'t|don\'t|doesn\'t|didn\'t|can\'t|couldn\'t|won\'t|wouldn\'t|shouldn\'t|haven\'t|hasn\'t|hadn\'t|ain\'t|n't)\b", re.IGNORECASE)
SARCASM_RE = re.compile(r'\b(yeah right|as if|sure|totally|what a joke)\b|\/s|\blol\b|\bsmh\b', re.IGNORECASE)
EMOJI_RE = re.compile(r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF]')  # basic emoji ranges

def caps_ratio(t):
    letters = [c for c in t if c.isalpha()]
    if not letters: return 0.0
    return sum(1 for c in letters if c.isupper()) / len(letters)

df_eval = pd.DataFrame({
    "text": val_texts,
    "true_id": true_ids,
    "pred_id": pred_ids,
    "true": [inv_label_map[i] for i in true_ids],
    "pred": [inv_label_map[i] for i in pred_ids],
    "conf": probs.max(axis=1),
    "len_words": [word_len(t) for t in val_texts],
    "has_negation": [bool(NEGATION_RE.search(t)) for t in val_texts],
    "exclam_count": [t.count("!") for t in val_texts],
    "caps_ratio": [caps_ratio(t) for t in val_texts],
    "has_sarcasm": [bool(SARCASM_RE.search(t)) for t in val_texts],
    "has_emoji": [bool(EMOJI_RE.search(t)) for t in val_texts],
    # Simple domain keywords (food dataset still varies nicely)
    "is_pet": [bool(re.search(r'\b(dog|cat|puppy|kitten|labrador|pet)\b', t, re.I)) for t in val_texts],
    "is_coffee": [bool(re.search(r'\b(coffee|espresso|caffeine|latte)\b', t, re.I)) for t in val_texts],
    "is_candy": [bool(re.search(r'\b(candy|taffy|chocolate|sweets|sweet)\b', t, re.I)) for t in val_texts],
})

# Length buckets
df_eval["len_bucket"] = pd.cut(
    df_eval["len_words"],
    bins=[0, 30, 80, np.inf],
    labels=["short(<=30)", "medium(31-80)", "long(>80)"],
    include_lowest=True
)

# ---------- 3) Helper: compute metrics for a boolean mask ----------
def slice_metrics(mask, name):
    idx = np.where(mask)[0]
    if len(idx) == 0:
        return {"slice": name, "n": 0, "acc": np.nan, "p_macro": np.nan, "r_macro": np.nan, "f1_macro": np.nan}
    y_true = df_eval.loc[idx, "true_id"].to_numpy()
    y_pred = df_eval.loc[idx, "pred_id"].to_numpy()
    acc = accuracy_score(y_true, y_pred)
    p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred, average="macro", zero_division=0)
    return {"slice": name, "n": len(idx), "acc": acc, "p_macro": p, "r_macro": r, "f1_macro": f1}

# ---------- 4) Evaluate important slices ----------
slices = []

# a) Length slices
for lbl in ["short(<=30)", "medium(31-80)", "long(>80)"]:
    slices.append(slice_metrics(df_eval["len_bucket"] == lbl, f"length={lbl}"))

# b) Negation present vs absent
slices.append(slice_metrics(df_eval["has_negation"], "negation=present"))
slices.append(slice_metrics(~df_eval["has_negation"], "negation=absent"))

# c) Emphasis: ALL-CAPS heavy, many exclamations, emojis
slices.append(slice_metrics(df_eval["caps_ratio"] >= 0.2, "caps_ratio>=0.2"))
slices.append(slice_metrics(df_eval["exclam_count"] >= 2, "exclam>=2"))
slices.append(slice_metrics(df_eval["has_emoji"], "has_emoji"))

# d) Sarcasm heuristic
slices.append(slice_metrics(df_eval["has_sarcasm"], "sarcasm_heuristic"))

# e) Domain keywords
slices.append(slice_metrics(df_eval["is_pet"], "domain=pet"))
slices.append(slice_metrics(df_eval["is_coffee"], "domain=coffee"))
slices.append(slice_metrics(df_eval["is_candy"], "domain=candy"))

metrics_df = pd.DataFrame(slices).sort_values(by=["slice"]).reset_index(drop=True)
print("=== Slice Metrics (macro-averaged) ===")
display(metrics_df)

# ---------- 5) Show representative failure cases per slice ----------
def show_failures(mask, k=5, want="FN/FP"):
    sub = df_eval[mask & (df_eval["true_id"] != df_eval["pred_id"])].copy()
    if sub.empty:
        print("No failures for this slice.")
        return
    # Sort by high confidence wrong predictions (model was confident but wrong)
    sub["margin"] = sub["conf"]
    sub = sub.sort_values(by="margin", ascending=False).head(k)
    cols = ["true", "pred", "conf", "len_words", "text"]
    display(sub[cols])

print("\n--- Failures: NEGATION present (most common pitfall) ---")
show_failures(df_eval["has_negation"], k=6)

print("\n--- Failures: Sarcasm heuristic ---")
show_failures(df_eval["has_sarcasm"], k=6)

print("\n--- Failures: Short reviews (<=30 words) ---")
show_failures(df_eval["len_bucket"] == "short(<=30)", k=6)

print("\n--- Failures: CAPS-heavy or many exclamations ---")
show_failures((df_eval["caps_ratio"] >= 0.2) | (df_eval["exclam_count"] >= 2), k=6)

# Optional: aggregate per-class F1 by slice (example: length buckets)
def per_class_f1_by_slice(slice_mask, name):
    from sklearn.metrics import classification_report
    idx = np.where(slice_mask)[0]
    if len(idx) == 0:
        print(f"\nNo examples for {name}")
        return
    y_true = df_eval.loc[idx, "true_id"].to_numpy()
    y_pred = df_eval.loc[idx, "pred_id"].to_numpy()
    print(f"\n=== Per-class report for {name} ===")
    print(classification_report(y_true, y_pred, target_names=["negative","neutral","positive"], zero_division=0))

per_class_f1_by_slice(df_eval["has_negation"], "negation=present")
per_class_f1_by_slice(df_eval["len_bucket"] == "short(<=30)", "short reviews")

!pip install gensim

!python -m spacy download en_core_web_md

import spacy
nlp = spacy.load("en_core_web_md")

# Example similarities
print("Similarity (good vs bad):", nlp("good").similarity(nlp("bad")))
print("Similarity (coffee vs tea):", nlp("coffee").similarity(nlp("tea")))
print("Similarity (dog vs cat):", nlp("dog").similarity(nlp("cat")))

!pip install torchtext

import spacy
nlp = spacy.load("en_core_web_sm")

# Similarities
print("good vs bad:", nlp("good").similarity(nlp("bad")))
print("coffee vs tea:", nlp("coffee").similarity(nlp("tea")))
print("dog vs cat:", nlp("dog").similarity(nlp("cat")))

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import TruncatedSVD
import numpy as np
from numpy.linalg import norm

# Use a small sample of reviews
sample_texts = df_final['Text'].dropna().sample(3000, random_state=42).tolist()

# Bag-of-words co-occurrence
vectorizer = CountVectorizer(max_features=5000, stop_words='english')
X = vectorizer.fit_transform(sample_texts)

# Reduce dimensions to create embeddings (100d)
svd = TruncatedSVD(n_components=100, random_state=42)
word_vectors = svd.fit_transform(X.T)

# Map vocab to embeddings
vocab = vectorizer.get_feature_names_out()
embeddings = {word: word_vectors[i] for i, word in enumerate(vocab)}

# Cosine similarity function
def cosine_similarity(w1, w2):
    if w1 not in embeddings or w2 not in embeddings:
        return None
    v1, v2 = embeddings[w1], embeddings[w2]
    return np.dot(v1, v2) / (norm(v1) * norm(v2))

# Test similarities
print("good vs bad:", cosine_similarity("good", "bad"))
print("coffee vs tea:", cosine_similarity("coffee", "tea"))
print("dog vs cat:", cosine_similarity("dog", "cat"))

def analogy(a, b, c, topn=1):
    if a not in embeddings or b not in embeddings or c not in embeddings:
        return None
    vec = embeddings[a] - embeddings[b] + embeddings[c]
    sims = {}
    for word, v in embeddings.items():
        if word in [a, b, c]:
            continue
        sims[word] = np.dot(vec, v) / (norm(vec) * norm(v))
    return sorted(sims.items(), key=lambda x: x[1], reverse=True)[:topn]

print("king - man + woman =", analogy("king", "man", "woman"))
print("paris - france + italy =", analogy("paris", "france", "italy"))

from sklearn.model_selection import train_test_split

X = df_balanced['Text']
y = df_balanced['label']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline

# Build pipeline
lr_pipeline = Pipeline([
    ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1,2))),
    ('clf', LogisticRegression(max_iter=1000, class_weight='balanced'))
])

# Train
lr_pipeline.fit(X_train, y_train)

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score
import matplotlib.pyplot as plt
import seaborn as sns

# Predictions
y_pred = lr_pipeline.predict(X_test)
y_probs = lr_pipeline.predict_proba(X_test)

# Metrics
acc = accuracy_score(y_test, y_pred)
print(f"Accuracy: {acc:.4f}")

print("Classification Report:\n", classification_report(
    y_test, y_pred, target_names=['negative','neutral','positive'])
)

# ROC-AUC (macro)
roc_auc = roc_auc_score(y_test, y_probs, multi_class='ovr')
print(f"ROC-AUC: {roc_auc:.4f}")

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=['negative','neutral','positive'],
            yticklabels=['negative','neutral','positive'])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix - Logistic Regression")
plt.show()





